{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:35.495942Z","iopub.execute_input":"2022-06-30T12:53:35.496403Z","iopub.status.idle":"2022-06-30T12:53:35.530921Z","shell.execute_reply.started":"2022-06-30T12:53:35.496312Z","shell.execute_reply":"2022-06-30T12:53:35.530104Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport time \nimport tensorflow as tf\nfrom tensorflow.compat import v1\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import Reshape\nfrom matplotlib import pyplot as plt\nfrom matplotlib.image import imread\nfrom glob import glob\nfrom PIL import Image\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:35.532580Z","iopub.execute_input":"2022-06-30T12:53:35.532825Z","iopub.status.idle":"2022-06-30T12:53:41.292851Z","shell.execute_reply.started":"2022-06-30T12:53:35.532792Z","shell.execute_reply":"2022-06-30T12:53:41.292024Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"## I am reading the csv file using a pandas dataframe adn then adding a columnn name file_path containing the image\n## path for the corresponding entry in the dataset\n\narr = os.listdir(\"/kaggle/input\")\ndata_dir = os.path.join(\"/kaggle/input\", arr[0])\nlist_dir = os.listdir(data_dir)\npath_for_train_dataset = os.path.join(data_dir, list_dir[1])\ndataframe = pd.read_csv(path_for_train_dataset)\ndataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:41.294827Z","iopub.execute_input":"2022-06-30T12:53:41.295171Z","iopub.status.idle":"2022-06-30T12:53:41.341462Z","shell.execute_reply.started":"2022-06-30T12:53:41.295119Z","shell.execute_reply":"2022-06-30T12:53:41.340476Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"del list_dir, path_for_train_dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:41.343260Z","iopub.execute_input":"2022-06-30T12:53:41.343530Z","iopub.status.idle":"2022-06-30T12:53:41.347350Z","shell.execute_reply.started":"2022-06-30T12:53:41.343493Z","shell.execute_reply":"2022-06-30T12:53:41.346483Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## train file list\ntrain_file = glob(os.path.join(data_dir, \"imgs/train/*/*.jpg\"))\n\n## Now we have a list of images of training dataset, let us add a path to the training pandas dataframe \ndataframe[\"file_path\"] = dataframe.apply(lambda x : os.path.join(data_dir, 'imgs/train', x.classname, x.img), axis = 1)\ndataframe[\"labels\"] = dataframe[\"classname\"].map(lambda x : int(x[1]))\ndataframe.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:41.349854Z","iopub.execute_input":"2022-06-30T12:53:41.350259Z","iopub.status.idle":"2022-06-30T12:53:44.850448Z","shell.execute_reply.started":"2022-06-30T12:53:41.350221Z","shell.execute_reply":"2022-06-30T12:53:44.849737Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"num_dataset = len(dataframe)\nprint(\"The number of Images in the dataset :\", num_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:44.851810Z","iopub.execute_input":"2022-06-30T12:53:44.852084Z","iopub.status.idle":"2022-06-30T12:53:44.857243Z","shell.execute_reply.started":"2022-06-30T12:53:44.852047Z","shell.execute_reply":"2022-06-30T12:53:44.856554Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## Now we have a Dataframe of the dataset. Let us use it to visualize the dataset and plotting relative graphs\n## Some raw visualisation led me to find that there are diffent photos of a single person.\n## That is let us find the subject \narr = dataframe[\"subject\"].value_counts()\nplt.bar(arr.index, arr.values)\nplt.grid()\nplt.xticks(rotation = 90)\nplt.xlabel(\"Subjects\")\nplt.ylabel(\"Number of images\")\nplt.show()\n\nprint(\"Number of Different Subject :\", len(arr.index))\nprint(\"Average Number of Images per subject :\", arr.values.sum()//len(arr.index))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:44.858701Z","iopub.execute_input":"2022-06-30T12:53:44.859216Z","iopub.status.idle":"2022-06-30T12:53:45.200515Z","shell.execute_reply.started":"2022-06-30T12:53:44.859176Z","shell.execute_reply":"2022-06-30T12:53:45.199762Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## So this survey has been done on 26 differnet subject \n## Each subject has nearly 862 images each\n## Let us also make a visualisation about the labels on the dataset.\narr = dataframe[\"labels\"].value_counts()\nplt.bar(arr.index, arr.values)\nplt.grid()\nplt.xticks(rotation = 90)\nplt.xlabel(\"Labels\")\nplt.ylabel(\"Number of images\")\nplt.show()\n\nprint(\"Number of labels :\", len(arr.index))\nprint(\"Average Number of Images per Label :\", arr.values.sum()/len(arr.index))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:45.201950Z","iopub.execute_input":"2022-06-30T12:53:45.202364Z","iopub.status.idle":"2022-06-30T12:53:45.400512Z","shell.execute_reply.started":"2022-06-30T12:53:45.202323Z","shell.execute_reply":"2022-06-30T12:53:45.397779Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## Let us now create the dataset\n## I saw that the size of the images is \n## So using a library cv2, i resized the images to 256*256\ndataset = list()\nlabels = list()\nfor i in range(num_dataset)  :\n    img = cv2.imread(dataframe[\"file_path\"][i])\n    resized = cv2.resize(img, (256, 256), cv2.INTER_LINEAR)\n    dataset.append(resized)\n    labels.append(int(dataframe[\"classname\"][i][1]))\n    \ndataset = np.array(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:53:45.401979Z","iopub.execute_input":"2022-06-30T12:53:45.402223Z","iopub.status.idle":"2022-06-30T12:58:08.686655Z","shell.execute_reply.started":"2022-06-30T12:53:45.402189Z","shell.execute_reply":"2022-06-30T12:58:08.685885Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## Let us look at the shape\ndataset.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:08.688138Z","iopub.execute_input":"2022-06-30T12:58:08.689013Z","iopub.status.idle":"2022-06-30T12:58:08.696104Z","shell.execute_reply.started":"2022-06-30T12:58:08.688964Z","shell.execute_reply":"2022-06-30T12:58:08.695239Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## so there are 22424 images adn the size of each image has been reduced to 256*256*3\n## Now let us see what is the shape of the labels\nlabels = np.array(labels)\nlabels.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:08.699934Z","iopub.execute_input":"2022-06-30T12:58:08.700659Z","iopub.status.idle":"2022-06-30T12:58:08.712881Z","shell.execute_reply.started":"2022-06-30T12:58:08.700613Z","shell.execute_reply":"2022-06-30T12:58:08.712069Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:08.714582Z","iopub.execute_input":"2022-06-30T12:58:08.714913Z","iopub.status.idle":"2022-06-30T12:58:08.723017Z","shell.execute_reply.started":"2022-06-30T12:58:08.714856Z","shell.execute_reply":"2022-06-30T12:58:08.722128Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"## Now let us split the  dataset into test and train with 30 percent of the data separated as test\n## We are going to use the train_test_split in the sklearn library\nx_train, x_test, y_train, y_test = train_test_split(dataset, labels, random_state = 10, test_size = 0.2)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:08.724626Z","iopub.execute_input":"2022-06-30T12:58:08.724973Z","iopub.status.idle":"2022-06-30T12:58:12.154470Z","shell.execute_reply.started":"2022-06-30T12:58:08.724934Z","shell.execute_reply":"2022-06-30T12:58:12.153749Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"## Now we need to One Hot Encode the labels in the y_train \n## So i am using the OneHotEncoder in sklearn \nencoder = OneHotEncoder(sparse = False)\ny_train = encoder.fit_transform(y_train.reshape((17939, 1)))\ny_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:12.155900Z","iopub.execute_input":"2022-06-30T12:58:12.156134Z","iopub.status.idle":"2022-06-30T12:58:12.168680Z","shell.execute_reply.started":"2022-06-30T12:58:12.156101Z","shell.execute_reply":"2022-06-30T12:58:12.167750Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"## The Random - access Memory was filled up. Now that the training and testing dataset is created, we can \n## delete the previous dataframe and the list of dataset to be deleted\ndel dataset\ndel dataframe\ndel labels","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:12.170408Z","iopub.execute_input":"2022-06-30T12:58:12.170740Z","iopub.status.idle":"2022-06-30T12:58:12.183949Z","shell.execute_reply.started":"2022-06-30T12:58:12.170703Z","shell.execute_reply":"2022-06-30T12:58:12.182672Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def create_model(input_shape, n_classes) :\n    model = Sequential()\n    model.add(Conv2D(32, (4, 4), padding = \"same\", activation = \"relu\", input_shape = input_shape))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    \n    model.add(Conv2D(64, (4, 4), padding = \"same\", activation = \"relu\"))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    \n    model.add(Conv2D(128, (4, 4), padding = \"same\", activation = \"relu\"))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    \n    model.add(Conv2D(256, (4, 4), padding = \"same\", activation = \"relu\"))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    \n    #model.add(Dropout(0.2))\n    \n    ##model.add(Flatten())\n    ##model.add(Dense(1024, activation = \"relu\"))\n    ##model.add(Reshape((32, 32, 1), input_shape = (1024, )))\n    \n    model.add(Conv2D(256, (4, 4), padding = \"same\", activation = \"relu\"))\n    model.add(Conv2D(128, (4, 4), padding = \"same\", activation = \"relu\"))\n    model.add(Conv2D(64, (4, 4), padding = \"same\", activation = \"relu\"))\n    model.add(Conv2D(32, (4, 4), padding = \"same\", activation = \"relu\"))\n    model.add(MaxPooling2D(pool_size = (2, 2)))\n    \n    model.add(Dropout(0.2))\n    \n    model.add(Flatten())\n    model.add(Dense(2048, activation = \"relu\"))\n    model.add(Dense(4096, activation = \"relu\"))\n    model.add(Dense(n_classes, activation = \"softmax\"))\n    \n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:12.186416Z","iopub.execute_input":"2022-06-30T12:58:12.187311Z","iopub.status.idle":"2022-06-30T12:58:12.199670Z","shell.execute_reply.started":"2022-06-30T12:58:12.187255Z","shell.execute_reply":"2022-06-30T12:58:12.198912Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"input_shape = (256, 256, 3)\nn_classes = 10\nmodel = create_model(input_shape, n_classes)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:12.201038Z","iopub.execute_input":"2022-06-30T12:58:12.201468Z","iopub.status.idle":"2022-06-30T12:58:14.662822Z","shell.execute_reply.started":"2022-06-30T12:58:12.201427Z","shell.execute_reply":"2022-06-30T12:58:14.662135Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## Now that we have created our model let us train an dtun it on the dataset \n## I am going to use cross validation and the accuracy metric\n## Firstly splitting 10 percent of the dataset to get x_validation and y_validation\nx_train_val, x_val, y_train_val, y_val = train_test_split(x_train, y_train, test_size = 0.1)\ndel x_train, y_train\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:14.664054Z","iopub.execute_input":"2022-06-30T12:58:14.665620Z","iopub.status.idle":"2022-06-30T12:58:15.778072Z","shell.execute_reply.started":"2022-06-30T12:58:14.665588Z","shell.execute_reply":"2022-06-30T12:58:15.777353Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## I am going to use the cross_entropy as teh loss function adn the adam optfimizer \n## Also let us define the number of iterations i.e. epochs and batch_size \nbatch_size = 100\nepochs = 25\n \n## Let us compile the model \nmodel.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\n## Fitting the training data and training as per the mentioned validation data, batch_size, metrics, optimizers etc.\nmodel_history = model.fit(x_train_val, y_train_val, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:58:15.779325Z","iopub.execute_input":"2022-06-30T12:58:15.779563Z","iopub.status.idle":"2022-06-30T13:13:43.986520Z","shell.execute_reply.started":"2022-06-30T12:58:15.779530Z","shell.execute_reply":"2022-06-30T13:13:43.984755Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"## The accuracy on Training data is 98.56 % which may suggest overfiiting but the validation accuracy is also nearly 98.7 % \n## So let us try the model on testing data\n## Before doing that, we need to free up ram space, so we can now delete training and validation data \n## As there is no longer a need fot those datasets\ndel x_train_val\ndel y_train_val\ndel x_val\ndel y_val\n\n## Collecting garbage memory\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:13:43.992548Z","iopub.execute_input":"2022-06-30T13:13:43.993898Z","iopub.status.idle":"2022-06-30T13:13:44.415406Z","shell.execute_reply.started":"2022-06-30T13:13:43.993848Z","shell.execute_reply":"2022-06-30T13:13:44.414745Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_test)\ny_pred = np.argmax(y_pred, 1)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:13:44.416721Z","iopub.execute_input":"2022-06-30T13:13:44.417017Z","iopub.status.idle":"2022-06-30T13:13:48.708428Z","shell.execute_reply.started":"2022-06-30T13:13:44.416981Z","shell.execute_reply":"2022-06-30T13:13:48.707725Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:13:48.709909Z","iopub.execute_input":"2022-06-30T13:13:48.710163Z","iopub.status.idle":"2022-06-30T13:13:48.715582Z","shell.execute_reply.started":"2022-06-30T13:13:48.710127Z","shell.execute_reply":"2022-06-30T13:13:48.714940Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Test result Validation","metadata":{}},{"cell_type":"code","source":"## Accuracy Score\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:13:48.716990Z","iopub.execute_input":"2022-06-30T13:13:48.717455Z","iopub.status.idle":"2022-06-30T13:13:48.732182Z","shell.execute_reply.started":"2022-06-30T13:13:48.717391Z","shell.execute_reply":"2022-06-30T13:13:48.731459Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"## Confusion Matrix\nconfusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:13:48.733319Z","iopub.execute_input":"2022-06-30T13:13:48.733636Z","iopub.status.idle":"2022-06-30T13:13:48.750268Z","shell.execute_reply.started":"2022-06-30T13:13:48.733599Z","shell.execute_reply":"2022-06-30T13:13:48.749611Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"## Classification reports\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:13:48.751707Z","iopub.execute_input":"2022-06-30T13:13:48.751971Z","iopub.status.idle":"2022-06-30T13:13:48.772350Z","shell.execute_reply.started":"2022-06-30T13:13:48.751933Z","shell.execute_reply":"2022-06-30T13:13:48.771576Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"## Loss Curves\nplt.figure(figsize = [8,6])\nplt.plot(model_history.history['loss'],'r',linewidth = 3.0)\nplt.plot(model_history.history['val_loss'],'b',linewidth = 3.0)\nplt.legend(['Training loss', 'Validation Loss'],fontsize = 18)\nplt.xlabel('Epochs ',fontsize = 16)\nplt.ylabel('Loss',fontsize = 16)\nplt.title('Loss Curves',fontsize = 16)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:13:48.773348Z","iopub.execute_input":"2022-06-30T13:13:48.773650Z","iopub.status.idle":"2022-06-30T13:13:49.049644Z","shell.execute_reply.started":"2022-06-30T13:13:48.773614Z","shell.execute_reply":"2022-06-30T13:13:49.048982Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"## Accuracy Curves\nplt.figure(figsize = [8,6])\nplt.plot(model_history.history['accuracy'],'r',linewidth = 3.0)\nplt.plot(model_history.history['val_accuracy'],'b',linewidth = 3.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize = 18)\nplt.xlabel('Epochs ',fontsize = 16)\nplt.ylabel('Accuracy',fontsize = 16)\n\nplt.title('Accuracy Curves',fontsize = 16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:13:49.051112Z","iopub.execute_input":"2022-06-30T13:13:49.051405Z","iopub.status.idle":"2022-06-30T13:13:49.252910Z","shell.execute_reply.started":"2022-06-30T13:13:49.051364Z","shell.execute_reply":"2022-06-30T13:13:49.252229Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}